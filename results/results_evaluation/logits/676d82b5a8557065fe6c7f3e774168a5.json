{"eval_loss": 2.273174524307251, "eval_accuracy": 0.38375, "eval_pred_num_tokens": 17.690000534057617, "eval_true_num_tokens": 17.479999542236328, "eval_token_set_precision": 0.5627697794904415, "eval_token_set_recall": 0.5598459949994389, "eval_token_set_f1": 0.555969950426313, "eval_token_set_f1_sem": 0.023231293019058182, "eval_n_ngrams_match_1": 7.09, "eval_n_ngrams_match_2": 3.19, "eval_n_ngrams_match_3": 1.78, "eval_num_true_words": 14.4, "eval_num_pred_words": 14.41, "eval_bleu_score": 24.855021038628987, "eval_bleu_score_sem": 2.451761792933914, "eval_rouge_score": 0.5301423099829967, "eval_exact_match": 0.04, "eval_exact_match_sem": 0.01969463855669324, "eval_ada_emb_cos_sim_mean": 0.0, "eval_ada_emb_cos_sim_sem": 0.0, "eval_emb_cos_sim": 0.9971069097518921, "eval_emb_cos_sim_sem": 0.00047335010317296186, "eval_emb_top1_equal": 0.28125, "eval_emb_top1_equal_sem": 0.08075219744018285, "eval_perplexity": 9.710177131474621, "eval_runtime": 14.1823, "eval_samples_per_second": 7.051, "eval_steps_per_second": 0.282, "_eval_args": {"alias": "jxm/t5-base__llama-7b-chat__one-million-instructions__emb", "dataset": "anthropic_toxic_prompts", "num_samples": 100, "batch_size": 32, "embedder_model_name": "meta-llama/Llama-2-7b-chat-hf"}}